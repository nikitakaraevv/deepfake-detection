
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/02_train.ipynb

import sys
sys.path.append('../')

from datasets import get_dataset
from path import Path
from models  import get_model
from sklearn.metrics import accuracy_score
import torch
import os
from args import parse_args
import torch.nn.functional as F
#from torch.utils.tensorboard import SummaryWriter

args = parse_args()

ds = get_dataset(args)

train_loader = torch.utils.data.DataLoader(ds(csv_file=os.path.join(args.root_dir,args.csv_file), root_dir=args.root_dir, split='train',
        phase='train', crop_size=args.crop_size),
        batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)
val_loader = torch.utils.data.DataLoader(ds(csv_file=os.path.join(args.root_dir,args.csv_file), root_dir=args.root_dir, split='test',
        phase='test', crop_size=args.crop_size), batch_size=args.batch_size,
        shuffle=False, num_workers=args.workers, pin_memory=True)


use_cuda = torch.cuda.is_available()                   # check if GPU exists
device = torch.device("cuda" if use_cuda else "cpu")

cnn_encoder, rnn_decoder = get_model(args)

print("Using", torch.cuda.device_count(), "GPU!")
# Combine all EncoderCNN + DecoderRNN parameters
crnn_params = list(cnn_encoder.fc1.parameters()) + list(cnn_encoder.bn1.parameters()) + \
              list(cnn_encoder.fc2.parameters()) + list(cnn_encoder.bn2.parameters()) + \
              list(cnn_encoder.fc3.parameters()) + list(rnn_decoder.parameters())

optimizer = torch.optim.Adam(crnn_params, lr=args.lr)

#cnn_encoder, rnn_decoder = model
epoch_train_losses = []
epoch_train_scores = []
epoch_test_losses = []
epoch_test_scores = []

for epoch in range(args.epochs):
    cnn_encoder.train()
    rnn_decoder.train()

    losses = []
    scores = []
    N_count = 0   # counting total trained sample in one epoch
    for batch_idx, (X, y) in enumerate(train_loader):
        # distribute data to device
        X, y = X.to(device), y.to(device).view(-1, )

        N_count += X.size(0)

        optimizer.zero_grad()
        output = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)

        loss = F.cross_entropy(output, y)
        losses.append(loss.item())

        # to compute accuracy
        y_pred = torch.max(output, 1)[1]  # y_pred != output
        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())
        scores.append(step_score)         # computed on CPU

        loss.backward()
        optimizer.step()

        # show information
        if (batch_idx + 1) % args.log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}, Accu: {:.2f}%'.format(
                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(), 100 * step_score))

    cnn_encoder.eval()
    rnn_decoder.eval()

    test_loss = 0
    all_y = []
    all_y_pred = []
    with torch.no_grad():
        for X, y in val_loader:
            # distribute data to device
            X, y = X.to(device), y.to(device).view(-1, )

            output = rnn_decoder(cnn_encoder(X))

            loss = F.cross_entropy(output, y, reduction='sum')
            test_loss += loss.item()                 # sum up batch loss
            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability

            # collect all y and y_pred in all batches
            all_y.extend(y)
            all_y_pred.extend(y_pred)

    test_loss /= len(val_loader.dataset)

    # compute accuracy
    all_y = torch.stack(all_y, dim=0)
    all_y_pred = torch.stack(all_y_pred, dim=0)
    test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())

    # show information
    print('\nTest set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%\n'.format(len(all_y), test_loss, 100* test_score))
    train_losses, train_scores = losses, scores
    epoch_test_loss, epoch_test_score = test_loss, test_score

    epoch_train_losses.append(train_losses)
    epoch_train_scores.append(train_scores)
    epoch_test_losses.append(epoch_test_loss)
    epoch_test_scores.append(epoch_test_score)

    # save all train test results
    np.save(os.path.join(args.save_model_path, './epoch_training_losses.npy'), np.array(epoch_train_losses))
    np.save(os.path.join(args.save_model_path, './epoch_training_scores.npy', np.array(epoch_train_scores)))
    np.save(os.path.join(args.save_model_path, './epoch_test_loss.npy', np.array(epoch_test_losses)))
    np.save(os.path.join(args.save_model_path, './epoch_test_score.npy', np.array(epoch_test_scores)))

    # save Pytorch models of best record
    torch.save(cnn_encoder.state_dict(), os.path.join(args.save_model_path, 'cnn_encoder_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder
    torch.save(rnn_decoder.state_dict(), os.path.join(args.save_model_path, 'rnn_decoder_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder
    torch.save(optimizer.state_dict(), os.path.join(args.save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer
    print("Epoch {} model saved!".format(epoch + 1))
